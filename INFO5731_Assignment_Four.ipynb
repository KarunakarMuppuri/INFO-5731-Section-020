{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarunakarMuppuri/INFO-5731-Section-020/blob/main/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "be52d805-78a6-4d8e-841b-58ae9104ffcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Topics:\n",
            "1. Trying surface got apple tablet microsoft say like standpoint ipad\n",
            "2. Trying apple tablet like standpoint microsoft say ipad surface got\n",
            "3. Apple tablet microsoft ipad say standpoint like surface got trying\n",
            "4. Say like microsoft ipad tablet standpoint trying surface got apple\n",
            "5. Tablet like ipad standpoint trying microsoft surface got apple say\n",
            "6. Like ipad microsoft apple surface got standpoint trying tablet say\n",
            "7. Ipad standpoint trying say apple surface got tablet microsoft like\n",
            "8. Microsoft ipad standpoint trying surface got tablet apple say like\n",
            "9. Standpoint like surface got apple say microsoft tablet trying ipad\n",
            "10. Surface got standpoint say ipad like microsoft tablet apple trying\n",
            "\n",
            "Top 10 Clusters:\n",
            "Cluster 1    4\n",
            "Cluster 5    1\n",
            "Cluster 7    1\n",
            "Cluster 8    1\n",
            "Cluster 4    1\n",
            "Cluster 9    1\n",
            "Cluster 6    1\n",
            "Name: Max, dtype: int64\n",
            "\n",
            "Cluster Summaries:\n",
            "\n",
            "Cluster 1 Summary: Trying surface got apple tablet microsoft say like standpoint ipad\n",
            "0    I got this tablet after trying an apple ipad a...\n",
            "2    The Amazon Fire HD 10 tablet, 10.1\", 1080p Ful...\n",
            "3    I've used Fire 8. It's a good tablet. Nothing ...\n",
            "6    I got this tablet after trying an apple ipad a...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 2 Summary: Trying apple tablet like standpoint microsoft say ipad surface got\n",
            "Series([], Name: Clean_Text, dtype: object)\n",
            "\n",
            "Cluster 3 Summary: Apple tablet microsoft ipad say standpoint like surface got trying\n",
            "Series([], Name: Clean_Text, dtype: object)\n",
            "\n",
            "Cluster 4 Summary: Say like microsoft ipad tablet standpoint trying surface got apple\n",
            "7    I wanted a tablet that would allow my daughter...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 5 Summary: Tablet like ipad standpoint trying microsoft surface got apple say\n",
            "1    I had an older Fire 7, and it didnâ€™t have any ...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 6 Summary: Like ipad microsoft apple surface got standpoint trying tablet say\n",
            "9    This is great for the purpose intended. It's n...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 7 Summary: Ipad standpoint trying say apple surface got tablet microsoft like\n",
            "4    I ordered one of these for travel in my car on...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 8 Summary: Microsoft ipad standpoint trying surface got tablet apple say like\n",
            "5    The story on this tablet is the battery doesn'...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 9 Summary: Standpoint like surface got apple say microsoft tablet trying ipad\n",
            "8    I had a nice Kindle that allowed me to read in...\n",
            "Name: Clean_Text, dtype: object\n",
            "\n",
            "Cluster 10 Summary: Surface got standpoint say ipad like microsoft tablet apple trying\n",
            "Series([], Name: Clean_Text, dtype: object)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df['Clean_Text'])\n",
        "\n",
        "# Apply Latent Semantic Analysis (LSA)\n",
        "lsa = TruncatedSVD(n_components=10, random_state=42)\n",
        "X_lsa = lsa.fit_transform(X)\n",
        "\n",
        "def get_topic_title(cluster):\n",
        "    # Get the top 10 words for the given cluster\n",
        "    indices = X_lsa[:, cluster].argsort()[::-1][:10]\n",
        "    words = [list(vectorizer.vocabulary_.keys())[i] for i in indices]\n",
        "    return ' '.join(words).capitalize()\n",
        "\n",
        "print('Top 10 Topics:')\n",
        "for i in range(10):\n",
        "    print(f'{i+1}. {get_topic_title(i)}')\n",
        "\n",
        "clusters = lsa.transform(X)\n",
        "df_clusters = pd.DataFrame(clusters, columns=[f'Cluster {i+1}' for i in range(10)])\n",
        "df_clusters['Max'] = df_clusters.idxmax(axis=1)\n",
        "\n",
        "print('\\nTop 10 Clusters:')\n",
        "print(df_clusters['Max'].value_counts().head(10))\n",
        "\n",
        "print('\\nCluster Summaries:')\n",
        "for i in range(10):\n",
        "    print(f'\\nCluster {i+1} Summary: {get_topic_title(i)}')\n",
        "    print(df[df_clusters['Max'] == f'Cluster {i+1}']['Clean_Text'].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "vATjQNTY8buA",
        "outputId": "fed029d0-8308-4118-e5e4-badea7cfc750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "Accuracy: 1.0\n",
            " Precision: 1.0\n",
            " Recall: 1.0\n",
            " F1 score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Classifier:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 score: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The features used to evaluate the random forest and decision tree algorithms would be the same as in the original code \\nThe frequency of occurrence of words in the text data, obtained using the CountVectorizer class from the scikit-learn library. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# I have implemented Decision Tree and Random forest algorithms \n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df['Clean_Text'])\n",
        "y = df['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "dt_predictions = dt_classifier.predict(X_test)\n",
        "print('Random Forest Classifier:')\n",
        "print(f'Accuracy: {accuracy_score(y_test, rf_predictions)}\\nPrecision: {precision_score(y_test, rf_predictions, pos_label=\"positive\")}\\nRecall: {recall_score(y_test, rf_predictions, pos_label=\"positive\")}\\nF1 score: {f1_score(y_test, rf_predictions, pos_label=\"positive\")}')\n",
        "\n",
        "\n",
        "# Random Forest classifier with 5-folds cross validation\n",
        "rf_classifier = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=StratifiedKFold(n_splits=5), n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "rf_predictions = grid_search.predict(X_test)\n",
        "\n",
        "print('\\nRandom Forest Classifier:\\n'\n",
        "      f'Accuracy: {accuracy_score(y_test, rf_predictions)}\\n'\n",
        "      f'Precision: {precision_score(y_test, rf_predictions, pos_label=\"positive\")}\\n'\n",
        "      f'Recall: {recall_score(y_test, rf_predictions, pos_label=\"positive\")}\\n'\n",
        "      f'F1 score: {f1_score(y_test, rf_predictions, pos_label=\"positive\")}')\n",
        "\n",
        "\n",
        "'''The features used to evaluate the random forest and decision tree algorithms would be the same as in the original code \n",
        "The frequency of occurrence of words in the text data, obtained using the CountVectorizer class from the scikit-learn library. '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfvMKJjIXS5G",
        "outputId": "4a7d09d2-93aa-447b-a8af-e1b2944cab33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f9310093045f>:22: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  combined_df.fillna(combined_df.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set RMSE: 1505990627819833.8\n",
            "SalePrice column data: \n",
            "1460   -2.329348e+17\n",
            "1461   -2.502251e+17\n",
            "1462   -2.285849e+17\n",
            "1463   -2.138583e+17\n",
            "1464   -2.705832e+17\n",
            "            ...     \n",
            "2914   -2.116083e+17\n",
            "2915   -2.429156e+17\n",
            "2916   -2.271459e+17\n",
            "2917   -2.233136e+17\n",
            "2918   -2.500418e+17\n",
            "Name: SalePrice, Length: 1459, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f9310093045f>:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df['SalePrice'] = predictions\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "test_df['SalePrice'] = -1\n",
        "\n",
        "cols = list(test_df.columns)\n",
        "saleprice_col = cols.pop(cols.index('SalePrice'))\n",
        "cols.append(saleprice_col)\n",
        "test_df = test_df[cols]\n",
        "\n",
        "\n",
        "test_df['SalePrice'] = -1\n",
        "\n",
        "combined_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
        "\n",
        "combined_df.fillna(combined_df.mean(), inplace=True)\n",
        "\n",
        "combined_df = pd.get_dummies(combined_df)\n",
        "\n",
        "train_df = combined_df[combined_df['SalePrice'] != -1]\n",
        "test_df = combined_df[combined_df['SalePrice'] == -1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_df_scaled = scaler.fit_transform(train_df.drop(['SalePrice'], axis=1))\n",
        "test_df_scaled = scaler.transform(test_df.drop(['SalePrice'], axis=1))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_df_scaled, train_df['SalePrice'], test_size=0.2, random_state=42)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = lr.predict(X_val)\n",
        "\n",
        "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "print('Validation set RMSE:', rmse)\n",
        "\n",
        "predictions = lr.predict(test_df_scaled)\n",
        "\n",
        "'''Here it adds a new column called Sale price to the test.csv file, \n",
        "you may see other columns filled with 0 and 1's there are created so that test.csv file have same columns that of train.csv file'''\n",
        "test_df['SalePrice'] = predictions\n",
        "print(\"SalePrice column data: \")\n",
        "print(test_df['SalePrice'])\n",
        "\n",
        "test_df.to_csv('test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oEKBZohJ3XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}