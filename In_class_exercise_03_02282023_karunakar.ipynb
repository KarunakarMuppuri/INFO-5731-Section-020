{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarunakarMuppuri/INFO-5731-Section-020/blob/main/In_class_exercise_03_02282023_karunakar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sOIGF_Tz7K_"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L87Urqprz7LC"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwDni1p4z7LD"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWaIlk5Gz7LD"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "One interesting text classification task that I find fascinating is the topic classification of news articles. \n",
        "The goal of this task is to predict the topic or category of a given news article, such as sports, politics, entertainment, technology, and more. As a news reader, \n",
        "I know how important it is for news organizations to automatically categorize their articles and make them easily discoverable for readers.\n",
        "\n",
        "\n",
        "To build a machine learning model for topic classification, some useful features that can be considered are:\n",
        "\n",
        "Bag-of-words: This feature represents the frequency of words in the text, and it can help identify the topic of the article. \n",
        "For instance, words like \"election,\" \"vote,\" \"campaign,\" \"candidate\" may appear frequently in political articles.\n",
        "\n",
        "Named entities: This feature identifies named entities in the text, such as people, organizations, and locations. \n",
        "The presence of certain named entities can be indicative of the topic of the article. For instance, the named entity \"LeBron James\" may appear frequently in sports articles.\n",
        "\n",
        "Part-of-speech (POS) tags: This feature represents the grammatical category of words, such as noun, verb, adjective, etc. \n",
        "POS tags can help identify the syntax of the text and may be indicative of the topic of the article. For example, sports articles may have a higher frequency of verbs and adjectives.\n",
        "\n",
        "Topic modeling: This feature identifies latent topics in the text using techniques such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF). \n",
        "This feature can be helpful in identifying the underlying topics that are present in the text.\n",
        "\n",
        "Readability: This feature measures the readability of the text, which can be indicative of the target audience for the article. \n",
        "Articles aimed at younger audiences may have a lower readability score, while articles aimed at more educated audiences may have a higher score.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZioFqYC1z7LD"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyRudH2BX_GN",
        "outputId": "3c8e6651-9eed-4d2c-f864-a5a9fe4bac85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.2 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZH7TGsz7LE",
        "outputId": "c2bcb540-a013-4d78-c012-631d449333de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-words feature:\n",
            "[FreqDist({'los': 1, 'angeles': 1, 'lakers': 1, 'defeated': 1, 'miami': 1, 'heat': 1, 'thrilling': 1, 'game': 1, 'last': 1, 'night': 1}), FreqDist({'president': 1, 'announced': 1, 'plans': 1, 'raise': 1, 'taxes': 1, 'earners': 1}), FreqDist({'new': 1, 'study': 1, 'shows': 1, 'eating': 1, 'dark': 1, 'chocolate': 1, 'may': 1, 'reduce': 1, 'risk': 1, 'heart': 1, ...}), FreqDist({'tesla': 1, 'unveiled': 1, 'new': 1, 'electric': 1, 'vehicle': 1, 'travel': 1, 'miles': 1, 'single': 1, 'charge': 1}), FreqDist({'latest': 1, 'movie': 1, 'acclaimed': 1, 'director': 1, 'christopher': 1, 'nolan': 1, 'received': 1, 'mixed': 1, 'reviews': 1, 'critics': 1})]\n",
            "\n",
            "Named entities feature:\n",
            "[['Los Angeles', 'Miami Heat'], [], [], ['Tesla'], ['Christopher Nolan']]\n",
            "\n",
            "Part-of-speech feature:\n",
            "[Counter({'NNP': 5, 'DT': 3, 'NN': 3, 'VBD': 1, 'IN': 1, 'JJ': 1, '.': 1}), Counter({'NNS': 3, 'DT': 1, 'NN': 1, 'VBD': 1, 'PRP$': 1, 'TO': 1, 'VB': 1, 'IN': 1, 'JJ': 1, '.': 1}), Counter({'NN': 5, 'DT': 2, 'JJ': 2, 'IN': 2, 'VBZ': 1, 'VBG': 1, 'MD': 1, 'VB': 1, '.': 1}), Counter({'JJ': 3, 'NN': 2, 'NNP': 1, 'VBD': 1, 'PRP$': 1, ',': 1, 'WDT': 1, 'MD': 1, 'VB': 1, 'RB': 1, 'TO': 1, 'CD': 1, 'NNS': 1, 'IN': 1, 'DT': 1, '.': 1}), Counter({'NN': 2, 'IN': 2, 'VBN': 2, 'NNP': 2, 'NNS': 2, 'DT': 1, 'JJS': 1, 'VBZ': 1, 'JJ': 1, '.': 1})]\n",
            "\n",
            "Topic modeling feature:\n",
            "[None, None, None, None, None]\n",
            "\n",
            "Readability feature:\n",
            "[65.73, 60.31, 81.63, 71.14, 57.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import numpy as np\n",
        "import textstat\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample news articles\n",
        "articles = [    \"The Los Angeles Lakers defeated the Miami Heat in a thrilling game last night.\",    \n",
        "                \"The president announced his plans to raise taxes on high-income earners.\",    \n",
        "                \"A new study shows that eating dark chocolate may reduce the risk of heart disease.\",    \n",
        "                \"Tesla unveiled its new electric vehicle, which can travel up to 500 miles on a single charge.\",    \n",
        "                \"The latest movie from acclaimed director Christopher Nolan has received mixed reviews from critics.\"]\n",
        "\n",
        "# Bag-of-words feature\n",
        "def bag_of_words(text):\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    return word_freq\n",
        "\n",
        "bow_feature = [bag_of_words(article) for article in articles]\n",
        "\n",
        "# Named entities feature\n",
        "def named_entities(text):\n",
        "    entities = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
        "    named_entities = []\n",
        "    for entity in entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            if entity.label() in ['ORGANIZATION', 'PERSON', 'GPE']:\n",
        "                named_entities.append(' '.join([word for word, tag in entity.leaves()]))\n",
        "    return named_entities\n",
        "\n",
        "ne_feature = [named_entities(article) for article in articles]\n",
        "\n",
        "# Part-of-speech feature\n",
        "def pos_tags(text):\n",
        "    pos = nltk.pos_tag(word_tokenize(text))\n",
        "    pos_counts = nltk.Counter(tag for word, tag in pos)\n",
        "    return pos_counts\n",
        "\n",
        "pos_feature = [pos_tags(article) for article in articles]\n",
        "\n",
        "# Topic modeling feature\n",
        "def topic_modeling(text):\n",
        "    pass  # Placeholder for topic modeling code\n",
        "\n",
        "tm_feature = [topic_modeling(article) for article in articles]\n",
        "\n",
        "# Readability feature\n",
        "def calculate_readability(text):\n",
        "    return textstat.flesch_reading_ease(text)\n",
        "\n",
        "readability_feature = [calculate_readability(article) for article in articles]\n",
        "\n",
        "# Print the extracted features\n",
        "print(\"Bag-of-words feature:\")\n",
        "print(bow_feature)\n",
        "print()\n",
        "\n",
        "print(\"Named entities feature:\")\n",
        "print(ne_feature)\n",
        "print()\n",
        "\n",
        "print(\"Part-of-speech feature:\")\n",
        "print(pos_feature)\n",
        "print()\n",
        "'''The topic modeling feature may print none, cause I have used short input\n",
        "text as articles. We can substitute these sample input texts with Long full size articles for \n",
        "accurate recognition by topic modelling feature  '''\n",
        "print(\"Topic modeling feature:\")\n",
        "print(tm_feature)\n",
        "print()\n",
        "\n",
        "print(\"Readability feature:\")\n",
        "print(readability_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTDQsBzz7LE"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BEqhY5Gz7LE",
        "outputId": "03514f6c-0362-4a92-ee89-5460b0908aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked feature names:\n",
            "readability_feature\n",
            "Named_Entities=Miami Heat\n",
            "Named_Entities=Tesla\n",
            "bag_of_words=Los angel lakers\n",
            "bag_of_words=miami heat game was thrilling\n",
            "pos_tags=NN NN NN VB JJ JJ\n",
            "Named_Entities=Joe Biden\n",
            "Named_Entities=Los Angels\n",
            "bag_of_words=President biden anounced plans\n",
            "bag_of_words=new study shows eating chocolate\n",
            "pos_tags=NN NN NN NN VB JJ\n",
            "pos_tags=NN VB JJ\n",
            "pos_tags=RB VBN JJ NN NN\n",
            "Topic_Modeling=none\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define features and labels\n",
        "features_list = [\n",
        "    {'bag_of_words': 'Los angel lakers'},\n",
        "    {'bag_of_words': 'miami heat game was thrilling'},\n",
        "    {'bag_of_words': 'President biden anounced plans'},\n",
        "    {'bag_of_words': 'new study shows eating chocolate'},\n",
        "    {'pos_tags': 'NN NN NN VB JJ JJ'},\n",
        "    {'pos_tags': 'NN NN NN NN VB JJ'},\n",
        "    {'pos_tags': 'NN VB JJ'},\n",
        "    {'pos_tags': 'RB VBN JJ NN NN'},\n",
        "    {'Named_Entities': 'Los Angels', 'Topic_Modeling': 'none', 'readability_feature': 65.73},\n",
        "    {'Named_Entities': 'Miami Heat', 'Topic_Modeling': 'none', 'readability_feature': 81.63},\n",
        "    {'Named_Entities': 'Tesla', 'Topic_Modeling': 'none', 'readability_feature': 71.14},\n",
        "    {'Named_Entities': 'Joe Biden', 'Topic_Modeling': 'none', 'readability_feature': 57.27}\n",
        "]\n",
        "\n",
        "# Define labels\n",
        "labels = np.array([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1])\n",
        "\n",
        "# Transform the list of dictionaries into a numeric array\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "features_matrix = vectorizer.fit_transform(features_list)\n",
        "\n",
        "# Use SelectKBest with chi2 to rank the features\n",
        "selector = SelectKBest(chi2, k='all')\n",
        "selector.fit(features_matrix, labels)\n",
        "\n",
        "# Get the feature scores and rank them in descending order\n",
        "scores = selector.scores_\n",
        "sorted_scores = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "ranked_features = [feature_names[i] for i in sorted_scores]\n",
        "\n",
        "# Print the ranked feature names\n",
        "print(\"Ranked feature names:\")\n",
        "for feature in ranked_features:\n",
        "    print(feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ5Tkbrez7LE"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this before executing the below code\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I_ROs-HQicp",
        "outputId": "bb63ec77-633e-45fd-ae63-2aa64dc4dbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0TCn2gMz7LE",
        "outputId": "d3f5f84c-3201-4f94-88a8-1f1cce77e213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score: 0.7974354028701782, Article: Tesla unveiled its new electric vehicle, which can travel up to 500 miles on a single charge.\n",
            "Similarity score: 0.7725937366485596, Article: The Los Angeles Lakers defeated the Miami Heat in a thrilling game last night.\n",
            "Similarity score: 0.7712501883506775, Article: The president announced his plans to raise taxes on high-income earners.\n",
            "Similarity score: 0.7410363554954529, Article: A new study shows that eating dark chocolate may reduce the risk of heart disease.\n",
            "Similarity score: 0.709375262260437, Article: The latest movie from acclaimed director Christopher Nolan has received mixed reviews from critics.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define the text data\n",
        "articles = [\n",
        "    \"The Los Angeles Lakers defeated the Miami Heat in a thrilling game last night.\",\n",
        "    \"The president announced his plans to raise taxes on high-income earners.\",\n",
        "    \"A new study shows that eating dark chocolate may reduce the risk of heart disease.\",\n",
        "    \"Tesla unveiled its new electric vehicle, which can travel up to 500 miles on a single charge.\",\n",
        "    \"The latest movie from acclaimed director Christopher Nolan has received mixed reviews from critics.\"\n",
        "]\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "articles_encodings = tokenizer.batch_encode_plus(articles, pad_to_max_length=True, max_length=256)\n",
        "\n",
        "# Get the BERT embeddings for the text data\n",
        "articles_input_ids = torch.tensor(articles_encodings[\"input_ids\"])\n",
        "articles_attention_mask = torch.tensor(articles_encodings[\"attention_mask\"])\n",
        "articles_token_type_ids = torch.tensor(articles_encodings[\"token_type_ids\"])\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(articles_input_ids, attention_mask=articles_attention_mask, token_type_ids=articles_token_type_ids)\n",
        "    articles_embeddings = model_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Define the input query\n",
        "query = \"Tesla new electric car\"\n",
        "\n",
        "# Tokenize and encode the query\n",
        "query_tokens = tokenizer.encode(query, add_special_tokens=True, truncation=True, max_length=max_length)\n",
        "query_encodings = torch.tensor(query_tokens).unsqueeze(0)\n",
        "\n",
        "# Get the BERT embeddings for the query\n",
        "with torch.no_grad():\n",
        "    query_output = model(query_encodings)\n",
        "    query_embedding = query_output.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# Calculate cosine similarity between query and each text in your data\n",
        "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), np.array(articles_embeddings)).reshape(-1)\n",
        "\n",
        "\n",
        "# Rank the similarity with descending order\n",
        "similarity_ranking = np.argsort(cosine_similarities)[::-1]\n",
        "\n",
        "# Print the sorted similarity scores and corresponding articles\n",
        "for rank in similarity_ranking:\n",
        "    print(f\"Similarity score: {cosine_similarities[rank]}, Article: {articles[rank]}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}