{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarunakarMuppuri/INFO-5731-Section-020/blob/Info-assigmnet-2/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using a hashtag (you can use any hashtag) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fea795-f883-43ca-d471-20d30f7e98c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "\n",
            "<!--[if lt IE 7]> <html lang=\"en-us\" class=\"a-no-js a-lt-ie9 a-lt-ie8 a-lt-ie7\"> <![endif]-->\n",
            "<!--[if IE 7]>    <html lang=\"en-us\" class=\"a-no-js a-lt-ie9 a-lt-ie8\"> <![endif]-->\n",
            "<!--[if IE 8]>    <html lang=\"en-us\" class=\"a-no-js a-lt-ie9\"> <![endif]-->\n",
            "<!--[if gt IE 8]><!-->\n",
            "<html class=\"a-no-js\" lang=\"en-us\"><!--<![endif]--><head>\n",
            "<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<meta content=\"IE=edge,chrome=1\" http-equiv=\"X-UA-Compatible\"/>\n",
            "<title dir=\"ltr\">Amazon.com</title>\n",
            "<meta content=\"width=device-width\" name=\"viewport\"/>\n",
            "<link href=\"https://images-na.ssl-images-amazon.com/images/G/01/AUIClients/AmazonUI-3c913031596ca78a3768f4e934b1cc02ce238101.secure.min._V1_.css\" rel=\"stylesheet\"/>\n",
            "<script>\n",
            "\n",
            "if (true === true) {\n",
            "    var ue_t0 = (+ new Date()),\n",
            "        ue_csm = window,\n",
            "        ue = { t0: ue_t0, d: function() { return (+new Date() - ue_t0); } },\n",
            "        ue_furl = \"fls-na.amazon.com\",\n",
            "        ue_mid = \"ATVPDKIKX0DER\",\n",
            "        ue_sid = (document.cookie.match(/session-id=([0-9-]+)/) || [])[1],\n",
            "        ue_sn = \"opfcaptcha.amazon.com\",\n",
            "        ue_id = 'F2BQ7QV68BJQFS11NHJ2';\n",
            "}\n",
            "</script>\n",
            "</head>\n",
            "<body>\n",
            "<!--\n",
            "        To discuss automated access to Amazon data please contact api-services-support@amazon.com.\n",
            "        For information about migrating to our APIs refer to our Marketplace APIs at https://developer.amazonservices.com/ref=rm_c_sv, or our Product Advertising API at https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html/ref=rm_c_ac for advertising use cases.\n",
            "-->\n",
            "<!--\n",
            "Correios.DoNotSend\n",
            "-->\n",
            "<div class=\"a-container a-padding-double-large\" style=\"min-width:350px;padding:44px 0 !important\">\n",
            "<div class=\"a-row a-spacing-double-large\" style=\"width: 350px; margin: 0 auto\">\n",
            "<div class=\"a-row a-spacing-medium a-text-center\"><i class=\"a-icon a-logo\"></i></div>\n",
            "<div class=\"a-box a-alert a-alert-info a-spacing-base\">\n",
            "<div class=\"a-box-inner\">\n",
            "<i class=\"a-icon a-icon-alert\"></i>\n",
            "<h4>Enter the characters you see below</h4>\n",
            "<p class=\"a-last\">Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.</p>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"a-section\">\n",
            "<div class=\"a-box a-color-offset-background\">\n",
            "<div class=\"a-box-inner a-padding-extra-large\">\n",
            "<form action=\"/errors/validateCaptcha\" method=\"get\" name=\"\">\n",
            "<input name=\"amzn\" type=\"hidden\" value=\"iX/7C6gNg37IvdJsXl8Svw==\"/><input name=\"amzn-r\" type=\"hidden\" value=\"/product-reviews/B08BX7FV5L/ref=zg-bs_1232597011_cr_sccl_4/142-0881380-3195946?pd_rd_w=pMDOb&amp;content-id=amzn1.sym.7cc5d8c9-0275-49fe-af63-f984f7dc4330&amp;pf_rd_p=7cc5d8c9-0275-49fe-af63-f984f7dc4330&amp;pf_rd_r=W8BJ2R4R18FT61DQCVFD&amp;pd_rd_wg=VLy5Z&amp;pd_rd_r=fa728cfe-04f0-43ca-a6e9-13ad54ea26df&amp;pd_rd_i=B08BX7FV5L\"/>\n",
            "<div class=\"a-row a-spacing-large\">\n",
            "<div class=\"a-box\">\n",
            "<div class=\"a-box-inner\">\n",
            "<h4>Type the characters you see in this image:</h4>\n",
            "<div class=\"a-row a-text-center\">\n",
            "<img src=\"https://images-na.ssl-images-amazon.com/captcha/tinytuux/Captcha_meocapzynk.jpg\"/>\n",
            "</div>\n",
            "<div class=\"a-row a-spacing-base\">\n",
            "<div class=\"a-row\">\n",
            "<div class=\"a-column a-span6\">\n",
            "</div>\n",
            "<div class=\"a-column a-span6 a-span-last a-text-right\">\n",
            "<a onclick=\"window.location.reload()\">Try different image</a>\n",
            "</div>\n",
            "</div>\n",
            "<input autocapitalize=\"off\" autocomplete=\"off\" autocorrect=\"off\" class=\"a-span12\" id=\"captchacharacters\" name=\"field-keywords\" placeholder=\"Type characters\" spellcheck=\"false\" type=\"text\"/>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"a-section a-spacing-extra-large\">\n",
            "<div class=\"a-row\">\n",
            "<span class=\"a-button a-button-primary a-span12\">\n",
            "<span class=\"a-button-inner\">\n",
            "<button class=\"a-button-text\" type=\"submit\">Continue shopping</button>\n",
            "</span>\n",
            "</span>\n",
            "</div>\n",
            "</div>\n",
            "</form>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"a-divider a-divider-section\"><div class=\"a-divider-inner\"></div></div>\n",
            "<div class=\"a-text-center a-spacing-small a-size-mini\">\n",
            "<a href=\"https://www.amazon.com/gp/help/customer/display.html/ref=footer_cou?ie=UTF8&amp;nodeId=508088\">Conditions of Use</a>\n",
            "<span class=\"a-letter-space\"></span>\n",
            "<span class=\"a-letter-space\"></span>\n",
            "<span class=\"a-letter-space\"></span>\n",
            "<span class=\"a-letter-space\"></span>\n",
            "<a href=\"https://www.amazon.com/gp/help/customer/display.html/ref=footer_privacy?ie=UTF8&amp;nodeId=468496\">Privacy Policy</a>\n",
            "</div>\n",
            "<div class=\"a-text-center a-size-mini a-color-secondary\">\n",
            "          Â© 1996-2014, Amazon.com, Inc. or its affiliates\n",
            "          <script>\n",
            "           if (true === true) {\n",
            "             document.write('<img src=\"https://fls-na.amaz'+'on.com/'+'1/oc-csi/1/OP/requestId=F2BQ7QV68BJQFS11NHJ2&js=1\" />');\n",
            "           };\n",
            "          </script>\n",
            "<noscript>\n",
            "<img src=\"https://fls-na.amazon.com/1/oc-csi/1/OP/requestId=F2BQ7QV68BJQFS11NHJ2&amp;js=0\">\n",
            "</img></noscript>\n",
            "</div>\n",
            "</div>\n",
            "<script>\n",
            "    if (true === true) {\n",
            "        var head = document.getElementsByTagName('head')[0],\n",
            "            prefix = \"https://images-na.ssl-images-amazon.com/images/G/01/csminstrumentation/\",\n",
            "            elem = document.createElement(\"script\");\n",
            "        elem.src = prefix + \"csm-captcha-instrumentation.min.js\";\n",
            "        head.appendChild(elem);\n",
            "\n",
            "        elem = document.createElement(\"script\");\n",
            "        elem.src = prefix + \"rd-script-6d68177fa6061598e9509dc4b5bdd08d.js\";\n",
            "        head.appendChild(elem);\n",
            "    }\n",
            "    </script>\n",
            "</body></html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The headers for amazon wer scrape expires after sometime if DOM doesnt return any HTML structure \n",
        "# Please use new headers and try again. I'm  also gonna attach a copy of csv file in canvas just in case\n",
        "# Headers are expired.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    \"TE\": \"Trailers\",\n",
        "}\n",
        "URL = \"https://www.amazon.com/product-reviews/B08BX7FV5L/ref=zg-bs_1232597011_cr_sccl_4/142-0881380-3195946?pd_rd_w=pMDOb&content-id=amzn1.sym.7cc5d8c9-0275-49fe-af63-f984f7dc4330&pf_rd_p=7cc5d8c9-0275-49fe-af63-f984f7dc4330&pf_rd_r=W8BJ2R4R18FT61DQCVFD&pd_rd_wg=VLy5Z&pd_rd_r=fa728cfe-04f0-43ca-a6e9-13ad54ea26df&pd_rd_i=B08BX7FV5L\"\n",
        "\n",
        "def scrape_reviews(url: str) -> pd.DataFrame:\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    print(soup)\n",
        "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
        "    data = []\n",
        "\n",
        "    for review in reviews:\n",
        "        title = review.find(\"a\", {\"data-hook\": \"review-title\"}).text.strip()\n",
        "        rating = review.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.strip()\n",
        "        text = review.find(\"span\", {\"data-hook\": \"review-body\"}).text.strip()\n",
        "        data.append([title, rating, text])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"Title\", \"Rating\", \"Review Text\"])\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = scrape_reviews(URL)\n",
        "    df.to_csv(\"product_reviews.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e2f17d-6a11-428c-9317-ba79a13ae6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "df = pd.read_csv(\"product_reviews.csv\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    stopwords_list.remove('not')\n",
        "    tokens = [word for word in tokens if word not in stopwords_list]\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "    cleaned_text = \" \".join(lemmatized_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "df['Cleaned Text'] = df['Review Text'].apply(clean_text)\n",
        "\n",
        "df.to_csv(\"product_reviews.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6622f6c3-16e1-4d19-f32d-ac7ccfb925f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos = []\n",
        "    for token in doc:\n",
        "        pos.append((token.text, token.pos_))\n",
        "    pos_count = dict(zip(set(pos), [pos.count(i) for i in set(pos)]))\n",
        "    return pos, pos_count\n",
        "\n",
        "def constituency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return list(doc.sents)[0].root.lefts\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return list(doc.sents)[0].root.children\n",
        "\n",
        "def entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ not in ['OTHER', 'CARDINAL']:\n",
        "            entities.append((ent.text, ent.label_))\n",
        "    entity_count = {}\n",
        "    for entity in entities:\n",
        "        if entity[0] not in entity_count:\n",
        "            entity_count[entity[0]] = 1\n",
        "        else:\n",
        "            entity_count[entity[0]] += 1\n",
        "    entity_count_str = [f\"{entity}: {count}\" for entity, count in entity_count.items()]\n",
        "    return entities, entity_count_str\n",
        "\n",
        "with open('product_reviews.csv', 'r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    rows = list(reader)\n",
        "\n",
        "with open('product_reviews.csv', 'w', newline='') as file:\n",
        "    fieldnames = ['Title', 'Review Text', 'Cleaned Text', 'POS', 'POS Count', 'Constituency Parsing', 'Dependency Parsing', 'Entities', 'Entity Count']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in rows:\n",
        "        pos, pos_count = pos_tagging(row['Cleaned Text'])\n",
        "        constituency_parse = [str(child) for child in constituency_parsing(row['Cleaned Text'])]\n",
        "        dependency_parse = [str(child) for child in dependency_parsing(row['Cleaned Text'])]\n",
        "        entity_count = entity_recognition(row['Cleaned Text'])\n",
        "\n",
        "        writer.writerow({\n",
        "            'Title': row['Title'],\n",
        "            'Review Text': row['Review Text'],\n",
        "            'Cleaned Text': row['Cleaned Text'],\n",
        "            'POS': pos,\n",
        "            'POS Count': pos_count,\n",
        "            'Constituency Parsing': constituency_parse,\n",
        "            'Dependency Parsing': dependency_parse,\n",
        "            'Entities': entity_count[0],\n",
        "            'Entity Count': entity_count[1]\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constituency parsing involves breaking down a sentence into its phrases and creating a tree structure that shows the grammatical relationships between them. The root of the tree represents the whole sentence, and each node represents a phrase. For example, \"The cat in the hat sat on the mat\" can be broken down into a tree with branches representing the noun phrase and verb phrase. The noun phrase further breaks down into a determiner and a noun, while the verb phrase consists of a prepositional phrase and a noun phrase.\n",
        "\n",
        "On the other hand, dependency parsing involves identifying the grammatical relationships between the words in a sentence and creating a graph that represents these relationships. Each word is a node in the graph, and the relationships between them are shown by directed edges. For instance, in \"The cat in the hat sat on the mat,\" the main verb \"sat\" is the root of the tree, and the other words are connected to it according to their grammatical roles. \"Cat\" is connected to \"sat\" as its subject, while \"on\" is connected to \"sat\" as the preposition introducing the prepositional phrase \"on the mat.\" The edges are labeled with the grammatical relationship between the words."
      ],
      "metadata": {
        "id": "g7671jFn8vcC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "USSdXHuqnwv9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}